---
name: x-ai-news-researcher
description: Discover and analyze the latest AI news from X.com (Twitter). Use when the user wants to find AI industry trends, news, model releases, technology breakthroughs, or wants a daily AI news briefing. Triggers include "AI news", "AIåŠ¨æ€", "AIæ–°é—»", "today in AI", "AIè¶‹åŠ¿", "what's happening in AI", "Xä¸Šçš„AIæ¶ˆæ¯". Do NOT use for general X.com browsing, non-AI topics, or posting tweets.
---

<!-- ============================================================
     STATIC CONTEXT BLOCK â€” Layer 1 (cacheable, >1024 tokens)
     All instructions, methodology, decision trees, and templates.
     ============================================================ -->

# X AI News Researcher

## Purpose

Discover, curate, and analyze the latest AI news and insights from X.com, then optionally deep-dive into the most significant topics via the deep-research plugin.

**Default output language:** Chinese (Simplified)

## Decision Tree (Execute First)

```
Request Analysis
â”œâ”€ Simple tweet lookup?           â†’ STOP: Use bird CLI directly
â”œâ”€ Non-AI topic?                  â†’ STOP: Not applicable for this skill
â”œâ”€ AI news discovery/analysis?    â†’ CONTINUE
â”‚
Execution Mode
â”œâ”€ Quick scan (user says "å¿«é€Ÿæ‰«ä¸€çœ¼" / "browse only")
â”‚   â†’ Phases 1-3 only, present analysis, skip deep research
â”œâ”€ Full workflow (default)
â”‚   â†’ All 6 phases
â””â”€ Deep research only (user provides specific topics)
    â†’ Skip to Phase 5-6
```

---

## 6-Phase Workflow

### Phase 1: FETCH â€” Data Collection

Invoke the **bird-fetcher** agent via Task tool to collect data from X.com.

**Determine fetch mode based on user input:**
- If user provides **specific topics/keywords** (e.g., "Claude 4", "multimodal agents", "å¼€æºå¤§æ¨¡å‹") â†’ use **Topic Mode**
- If user has **no specific topic** (general AI news browsing) â†’ use **Default Mode**

**Default Mode â€” no user topic:**

```
Task(
  subagent_type = "bird-fetcher" (agent),
  prompt = """
  Execute pre-flight checks (install verification + auth check).
  Then fetch these two data sources IN PARALLEL:

  1. AI-curated news with related tweets:
     bird news --with-tweets --tweets-per-item 3 --ai-only -n 20 --json --plain --timeout 30000

  2. Home timeline (For You):
     bird home -n 30 --json --plain --timeout 30000

  Return both JSON results labeled as AI_NEWS_DATA and HOME_TIMELINE_DATA.
  """
)
```

**Topic Mode â€” user specifies topics of interest:**

```
Task(
  subagent_type = "bird-fetcher" (agent),
  prompt = """
  Execute pre-flight checks (install verification + auth check).
  Then fetch these data sources IN PARALLEL:

  1. AI-curated news with related tweets:
     bird news --with-tweets --tweets-per-item 3 --ai-only -n 20 --json --plain --timeout 30000

  2. Home timeline (For You):
     bird home -n 30 --json --plain --timeout 30000

  3. Targeted search for user topic:
     bird search "<user_topic>" -n 20 --json --plain --timeout 30000

  Return all JSON results labeled as AI_NEWS_DATA, HOME_TIMELINE_DATA, and SEARCH_DATA.
  """
)
```

**Error handling:**
- If bird-fetcher reports auth failure â†’ relay the auth setup instructions to the user, stop workflow.
- If bird-fetcher reports bird not installed â†’ relay installation status to user.
- If one source returns empty but the other succeeds â†’ proceed with available data.
- If both sources return empty â†’ report to user and stop.

---

### Phase 2: SAVE â€” Working Directory Setup & Raw Data Persistence

**Step 1:** Create working directory:
```bash
timestamp=$(date +%Y%m%d_%H%M%S)
work_dir="${CLAUDE_PROJECT_DIR}/x-ai-news-researcher_result_${timestamp}"
mkdir -p "${work_dir}"
```

**Step 2:** Save AI News data â†’ `${work_dir}/01_raw_ai_news.md`

Format each news item as:
```markdown
## [News Title / Headline]

- **Category**: [category field from bird news]
- **Time**: [timeAgo e.g. "21 hours ago"]
- **Post Count**: [postCount â€” number of posts about this topic]
- **URL**: [twitter://trending/... URL]

### Summary
[If --with-tweets was used, related tweets will be shown below. Otherwise, this is just the trending headline.]
```

**Note:** `bird news` returns aggregated trending topics, not individual tweets. To get actual tweet content with engagement metrics, use `--with-tweets --tweets-per-item N`. The tweet JSON fields are documented below in the Home Timeline format.

**Step 3:** Save Home Timeline data â†’ `${work_dir}/02_raw_home_timeline.md`

Format each tweet as:
```markdown
## Tweet by @username [Display Name]

- **Tweet ID**: [id]
- **Time**: [createdAt e.g. "Sat Feb 07 23:50:40 +0000 2026"]
- **URL**: https://x.com/@username/status/[id]
- **Engagement**: [likeCount] likes | [retweetCount] RTs | [replyCount] replies
- **Is Retweet**: [Yes if text starts with "RT", else No]
- **Is Reply**: [Yes if replyCount > 0 and not a self-reply, else No]

### Content
[Full text field content]

### Quoted Tweet (if quotedTweet exists)
#### Quoted from @quotedUsername
- **Content**: [quotedTweet.text]
- **Engagement**: [quotedTweet.likeCount] likes | [quotedTweet.retweetCount] RTs
```

**Design note:** AI NEWS and HOME TIMELINE are kept in separate files because they come from different signal sources and carry different baseline quality â€” Explore news is editorially curated by X, while the home timeline reflects the user's personal network.

**Step 4 (Topic Mode only):** Save Search data â†’ `${work_dir}/02b_raw_search_results.md`

Same format as Home Timeline. Label each entry with the search query that produced it.

**Step 5: Fetch Full Content for Valuable Tweets (Phase 2.5)**

Bird `home` and `news --with-tweets` may return truncated content. For tweets that appear valuable based on initial scan:

1. Identify tweets with high engagement OR from authoritative sources OR about significant topics
2. Collect their tweet IDs (numeric IDs like `2020865177525305840`)
3. Fetch full content via bird-fetcher agent:
```
Task(
  subagent_type = "bird-fetcher" (agent),
  prompt = """
  Fetch full content for these tweet IDs using bird read:
  zsh -c 'source ~/.zshrc && bird read 2020865177525305840 --json --plain'
  zsh -c 'source ~/.zshrc && bird read 2020865177525305841 --json --plain'
  ... (up to 10 IDs in parallel)

  Return FULL_TWEET_DATA with complete text, media, quoted tweets, and reply context.
  """
)
```

4. Append the full content to the respective markdown files (`01_raw_ai_news.md` or `02_raw_home_timeline.md`) under a `### Full Content (Fetched)` section for each enriched tweet.

**Priority for full fetch:**
- Tweets from known AI researchers/companies (OpenAI, Anthropic, Google DeepMind, etc.)
- Tweets with high engagement (likeCount > 500 OR retweetCount > 100)
- Tweets mentioning new models, papers, or product releases
- Tweets with truncated text ending in "..." (obviously cut off)

---

### Phase 3: ANALYZE â€” Content Analysis & Filtering

Read `01_raw_ai_news.md` and `02_raw_home_timeline.md`, apply LLM analysis.

#### Content Priority Taxonomy (descending priority)

| Priority | Category | ä¸­æ–‡ | Description | Signal Examples |
|----------|----------|------|-------------|-----------------|
| 1 | Industry Observation | è¡Œä¸šè§‚å¯Ÿ | Industry shifts, company strategy, ecosystem changes | Company announcements, partnerships, funding rounds, policy changes, acquisitions |
| 2 | Technical Innovation | æŠ€æœ¯åˆ›æ–° | New models, architectures, breakthroughs | Paper releases, benchmark records, new capabilities demo, open-source releases |
| 3 | AI Evaluation | AIè¯„æµ‹ | Benchmarks, comparisons, performance analysis | Head-to-head comparisons, real-world test results, user reviews with data |
| 4 | Deep Insight | æ·±åˆ»è®¤çŸ¥ | Profound observations, paradigm-level thinking | Thought leadership, contrarian analysis, mental models, industry predictions with reasoning |
| 5 | Technical Tips | æŠ€æœ¯æŠ€å·§ | Practical how-tos, prompting, tools, workflows | Tutorials, prompt engineering, code snippets, tool recommendations with demo |

#### Filtering Rules (MUST filter out)

- **Promotional / Ad tweets**: Sponsored content, product shilling, "use my referral link"
- **Self-promotional hooks**: "Follow me for more...", "Like & RT to help me grow"
- **Engagement farming**: Vague hot takes with no substance, ragebait, "unpopular opinion:" with nothing novel
- **Reposts of old news**: Check recency â€” if the underlying event is >7 days old, deprioritize
- **Non-AI content**: Content that passed through bird's AI filter but is actually about crypto, general tech drama, or pure memes without AI substance
- **Low-information tweets**: One-line reactions, emoji-only responses, "this is huge" with no elaboration

#### Ambiguous Concept Resolution

Only use WebSearch when a tweet contains **unrecognized terms or acronyms** that you genuinely cannot classify. Do NOT search for content that is clearly low-value regardless of terminology (e.g., "this is huge" or emoji-only reactions â€” these are low-information tweets, skip them directly).

**When to search:**
- The tweet mentions a specific product/model/term you don't recognize AND the tweet has meaningful context suggesting it could be important
- You cannot determine whether a term is AI-related or not, and the tweet's engagement or source suggests it might be valuable

**When NOT to search:**
- The tweet is obviously low-information regardless of unknown terms
- The term is clearly non-AI (crypto, gaming, politics)
- You already know the term from your training data

**If searching:**
1. Use **WebSearch** tool: `"<term>" AI` or `"<term>" what is`
2. Apply findings to re-classify. If non-AI or low-value â†’ filter out.

**Examples of terms that may need verification:**
- New model names (e.g., "Qwen 4", "DeepSeek V5")
- Benchmark acronyms (e.g., "MMLU", "HumanEval", "LMSYS")
- Technical terms (e.g., "MoE", "RLHF", "CoT")
- Product names from non-English sources


#### Analysis Output Format (per item)

```markdown
### [Title / Summary â€” one line]

- **Category**: [è¡Œä¸šè§‚å¯Ÿ|æŠ€æœ¯åˆ›æ–°|AIè¯„æµ‹|æ·±åˆ»è®¤çŸ¥|æŠ€æœ¯æŠ€å·§]
- **Source**: [@username] â€” [brief description of who they are if notable]
- **Source Type**: [AI News (Explore) | Home Timeline | Search Results]
- **Engagement**: [likeCount] likes | [retweetCount] RTs | [replyCount] replies
- **Tweet URL**: https://x.com/@username/status/[id]
- **Key Insight**: [1-2 sentence summary of WHY this matters]
- **Substance**: [High|Medium] â€” information density assessment
- **Deep-Dive Potential**: [Yes|No] â€” whether this warrants in-depth research
```

Save analysis results â†’ `${work_dir}/03_analysis_results.md`

---

### Phase 4: SELECT â€” Deep-Dive Topic Selection

From the analyzed results, select **up to 3** topics for potential deep research.

#### Selection Methodology (Weighted Scoring)

| Factor | Weight | How to Evaluate |
|--------|--------|-----------------|
| Content Substance | 40% | Actual information density, novelty, depth of the topic. Is there enough to research? |
| Engagement Metrics | 25% | Views, likes, RTs as social proof. High engagement on technical content = strong signal. |
| Publisher Authority | 20% | KOL / major company account / established researcher / verified expert. Weight known voices higher. |
| Timeliness | 15% | Is this breaking news or already widely covered? Prioritize fresh, under-explored angles. |

#### Selection Rules

- **Maximum 3 topics** â€” if fewer qualify, select fewer. Quality over quantity.
- **Diversity**: Topics should cover different aspects (not 3 variants of the same news).
- **Research-worthiness**: Each topic must have enough substance to warrant a 15+ minute deep research session. Simple announcements that are fully explained in the tweet don't qualify.
- **Topic reframing**: If a tweet is about a paper/product/event, the deep-research topic should be the underlying technology/trend, not the tweet itself. Example: A tweet about "GPT-5 released" â†’ research topic is "GPT-5 architecture, capabilities, and industry impact", not "analysis of @OpenAI's tweet".

#### Pragmatic & Precise Topic Formulation

**Be specific, not vague.** The research question must be concrete enough to yield actionable insights.

| Bad (Too Vague) | Good (Precise & Pragmatic) |
|:----------------|:---------------------------|
| "What's new in AI agents?" | "Current state of autonomous AI agents: capabilities, limitations, and real-world deployment challenges in 2025" |
| "Is GPT-5 better?" | "GPT-5 vs Claude 4 vs Gemini 2: comparative analysis on coding, reasoning, and multimodal capabilities based on independent benchmarks" |
| "AI video generation" | "Recent breakthroughs in AI video generation: Sora, Veo, Kling - technical comparison and practical applications for content creators" |
| "OpenAI news" | "OpenAI's product strategy shift under new leadership: analysis of recent releases, partnership changes, and competitive positioning" |

**Pragmatic filters to apply:**
1. **Actionability**: Will this research help the user make decisions (tool selection, investment, career planning)?
2. **Recency premium**: Breaking news from last 48 hours gets priority over already-covered topics
3. **Signal-to-noise**: Prefer topics with primary sources (papers, official docs) over opinion pieces
4. **User relevance**: If user specified interests, prioritize topics aligning with those interests

**Avoid these research topics:**
- Purely speculative rumors without evidence
- "X vs Y" where both are minor players without meaningful differentiation
- Company news that's just PR fluff with no technical substance
- Topics already extensively covered by major tech outlets in the past week (unless new angles emerged)

Save selections â†’ `${work_dir}/04_deep_dive_candidates.md`

Format:
```markdown
## Deep-Dive Candidate 1: [Topic Title]

- **Original Source**: [@handle] â€” [tweet URL]
- **Proposed Research Question**: [The actual question to research]
- **Research Type**: [technical | comparison | market | general]
- **Selection Score**: [Overall weighted score]
  - Substance: [score/10]
  - Engagement: [score/10]
  - Authority: [score/10]
  - Timeliness: [score/10]
- **Why This Matters**: [2-3 sentences on why this deserves deep research]
```

---

### Phase 5: PRESENT â€” User-Facing Summary

Present two sections to the user **in chat** (not to file).

#### Section A: AI æ–°é—»é€ŸæŠ¥

Display all items that passed the analysis filter, organized by priority category:

```
## ğŸ” AI æ–°é—»é€ŸæŠ¥ (YYYY-MM-DD)

### è¡Œä¸šè§‚å¯Ÿ
1. **[Title]** â€” [Key insight in one sentence]
   [@source | views/likes/RTs | [Tweet URL]]

2. ...

### æŠ€æœ¯åˆ›æ–°
1. ...

### AIè¯„æµ‹
1. ...

### æ·±åˆ»è®¤çŸ¥
1. ...

### æŠ€æœ¯æŠ€å·§
1. ...

---
ğŸ“Š å…±å‘ç° N æ¡æœ‰ä»·å€¼çš„ AI ä¿¡æ¯ | æ•°æ®æ¥æº: X.com AI News + Home Timeline
ğŸ“ åŸå§‹æ•°æ®å·²ä¿å­˜è‡³: ${work_dir}/
```

#### Section B: æ¨èæ·±ç ”ä¸»é¢˜

```
## ğŸ“Œ æ¨èæ·±å…¥ç ”ç©¶çš„ä¸»é¢˜

### 1. [Topic Title]
- **åŸå§‹æ¨æ–‡**: [@handle â€” tweet URL]
- **ç ”ç©¶é—®é¢˜**: [Formulated research question]
- **ä¸ºä»€ä¹ˆå€¼å¾—æ·±ç ”**: [reasoning â€” engagement + substance + authority]

### 2. [Topic Title]
...

### 3. [Topic Title]
...

---
æ˜¯å¦å¯¹ä»¥ä¸Šä¸»é¢˜è¿›è¡Œæ·±åº¦ç ”ç©¶ï¼Ÿ
- å›å¤ **"å…¨éƒ¨ç ”ç©¶"** â†’ å¹¶å‘ç ”ç©¶æ‰€æœ‰ä¸»é¢˜
- å›å¤ **"ç ”ç©¶ 1,3"** â†’ åªç ”ç©¶æŒ‡å®šä¸»é¢˜
- å›å¤ **"ä¸éœ€è¦"** â†’ ç»“æŸæœ¬æ¬¡ä»»åŠ¡
```

**Then use AskUserQuestion tool** to ask user whether to proceed with deep research.

---

### Phase 6: DEEP RESEARCH â€” Parallel Deep Dives (Conditional)

Only executes if user approves in Phase 5.

#### Step 1: Setup

- Use **TodoWrite** to create tracking checklist for all approved topics.
- Get the current date for folder naming:
  ```bash
  date_str=$(date +%Y%m%d)
  ```

#### Step 2: Parallel Execution via Task Tool

For each approved topic, launch a **Task tool** call with `subagent_type="general-purpose"`:

```
Task(
  subagent_type = "general-purpose",
  max_turns = 50,
  description = "Deep research: [topic title]",
  prompt = """
  Execute the deep-research skill on the following research question:

  [Formulated research question from Phase 4]

  Context: This research was triggered by an AI news item from X.com:
  - Original tweet: [URL]
  - Author: [@handle]
  - Key claim/observation: [summary]

  Configuration:
  - Research mode: standard
  - Research type: [technical|comparison|market|general]
  - Output language: Chinese

  IMPORTANT: Override the default project folder. When deep-research initializes its project folder, set:
    timestamp=$(date +%Y%m%d_%H%M%S)
    work_dir="${CLAUDE_PROJECT_DIR}/x-ai-news-researcher_result_${timestamp}
    date_str=$(date +%Y%m%d)
    project_folder="${work_dir}/${topic_slug}_Research_${date_str}/"
  
  All output files must be saved inside this overridden project folder.
  """
)
```

**Parallel execution rules:**
- Launch **up to 3** Task calls in a **single message** for maximum parallelism.
- Track progress via **TodoWrite** â€” mark each topic as in_progress / completed.
- If a research task fails, report the error but continue with other tasks.

#### Step 3: Post-Research Summary

After all deep research tasks complete, present:

```
## âœ… æ·±åº¦ç ”ç©¶å®Œæˆ

### ç ”ç©¶æŠ¥å‘Šæ±‡æ€»

| # | ä¸»é¢˜ | æŠ¥å‘Šè·¯å¾„ | çŠ¶æ€ |
|---|------|----------|------|
| 1 | [Topic] | ${work_dir}/${topic_slug}_Research_${date_str}/ | âœ… å®Œæˆ |
| 2 | [Topic] | ${work_dir}/${topic_slug}_Research_${date_str}/ | âœ… å®Œæˆ |
| 3 | [Topic] | ${work_dir}/${topic_slug}_Research_${date_str}/ | âŒ å¤±è´¥ (åŸå› ) |
```

**Note:** deep-research creates folders in the format `${topic_slug}_Research_${date_str}/`. All research reports will be organized as subdirectories under the main work directory.

---

## Error Handling Summary

| Scenario | Action |
|----------|--------|
| bird not installed | Auto-install via `npm install -g @steipete/bird`, retry |
| Cookie auth not configured | Show auth setup guide (env vars / CLI flags / browser cookies), stop |
| bird auth fails (401/403) | Show `bird check` instructions + cookie setup guide, stop |
| bird rate limited (429) | Report, suggest waiting 1-2 min, stop |
| AI News returns empty | Proceed with Home Timeline only |
| Home Timeline returns empty | Proceed with AI News only |
| Both sources empty | Report to user, stop |
| No valuable AI content found | Report honestly â€” "æœ¬æ¬¡æ‰«ææœªå‘ç°é«˜ä»·å€¼ AI ä¿¡æ¯", stop |
| Fewer than 3 deep-dive candidates | Present however many qualify (could be 0-2) |
| deep-research task fails | Report specific failure, continue with other tasks |

---

## Output Directory Contract

```
${CLAUDE_PROJECT_DIR}/x-ai-news-researcher_result_<YYYYMMDD_HHMMSS>/
â”œâ”€â”€ 01_raw_ai_news.md              # Phase 2: Raw AI news from X Explore
â”œâ”€â”€ 02_raw_home_timeline.md        # Phase 2: Raw home timeline data
â”œâ”€â”€ 02b_raw_search_results.md      # Phase 2: Raw search results (Topic Mode only)
â”œâ”€â”€ 03_analysis_results.md         # Phase 3: Analyzed & filtered results
â”œâ”€â”€ 04_deep_dive_candidates.md     # Phase 4: Selected deep-dive topics
â”œâ”€â”€ <topic_slug>_Research_<YYYYMMDD>/   # Phase 6: Deep research report 1 (deep-research naming)
â”‚   â”œâ”€â”€ research_report_<YYYYMMDD>_<topic_slug>.md
â”‚   â”œâ”€â”€ research_report_<YYYYMMDD>_<topic_slug>.html
â”‚   â””â”€â”€ reference/
â”œâ”€â”€ <topic_slug>_Research_<YYYYMMDD>/   # Phase 6: Deep research report 2
â”‚   â””â”€â”€ ...
â””â”€â”€ <topic_slug>_Research_<YYYYMMDD>/   # Phase 6: Deep research report 3
    â””â”€â”€ ...
```

<!-- End of STATIC CONTEXT BLOCK
     Above content is cacheable (>1024 tokens, static across invocations).
     ============================================================ -->

---

<!-- ============================================================
     DYNAMIC EXECUTION ZONE â€” Layer 3
     Content below is populated during runtime only.
     ============================================================ -->

## Dynamic Execution Zone

**User Query:**
[User request will be processed here during execution]

**Fetch Results:**
[Bird CLI results will be accumulated here]

**Analysis Results:**
[Filtered and ranked content will be generated here]

**Deep Research Status:**
[Research progress tracking will be maintained here via TodoWrite]
